---
title: "Analysis of Election Tweets"
author: "Andy Pickering"
date: "9/28/2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Being in the middle (or I guess thankfully near the end!) of election season, I thought it would be interesting to examine tweets about the election.

Much of this analysis applies what I learned in a DataCamp course on text-mining: <https://www.datacamp.com/courses/intro-to-text-mining-bag-of-words>.

To collect the tweets, i'm using the rtweet package. The twitter API only lets you access tweets for the previous 7 days, so i've downloaded a collection of tweets spanning the first presidential debate and saved it as a csv file. 

Note that you will need to register for a (free) token to access the Twitter API. The creator of the rtweet package created a guide to doing this here: <https://github.com/mkearney/rtweet/blob/master/vignettes/tokens.Rmd>.

The code used to download the tweets was:

```{r eval=FALSE}
library(rtweet)
# tweets with hashtag #debatenight from 9/25-9/28
tw1 <- search_tweets("#debatenight", n = 18000,since = "2016-09-25", until = "2016-09-28", token = twitter_token, lang = "en")
# save to csv file
save_as_csv(tw1,file_name='~/Tweets_Analysis/Data/debatenight_25_28')

```

Let's load one of the datasets and take a peek at it.
```{r}
tw <- read.csv('~/Tweets_Analysis/Data/debatenight_25_28.tweets.csv')
str(tw)
```

There are a lot of variables; let's clean it up a bit by removing some columns we won't use:
```{r}
tw2<-tw[,-c(7:10,15,16,20:23)]
str(tw2)
head(tw2)
```

More cleaning: keep only the tweets that were not retweets, and remove username mentions from the tweets: 
```{r}
library(dplyr)
tw2 <- dplyr::filter(tw2, is_retweet == FALSE) %>% mutate(text = gsub("\\@.*", "", text)) 
head(tw2)
```

## Cleaning Text and Making a Corpus

Let's take a look at the text of the first few tweets:
```{r}
head(tw2)$text
```

We need to clean up the tweets text before doing some analysis.. I'm going to use the tm package for this. Some of the cleaning includes: 

* Remove whitespace 
* Remove punctuation 
* Make all lowercase 
* Remove common words 


```{r}
library(tm)

# 1st need to interpret each element in tweets as a document
tweet_source <- VectorSource(tw$text)

# Then make a corpus
tweet_corpus <- VCorpus(tweet_source)

# Define a function to do some basic cleaning of the corpus text
clean_corpus <- function(corpus){
        corpus <- tm_map(corpus, stripWhitespace)
        corpus <- tm_map(corpus, removePunctuation)
        corpus <- tm_map(corpus, content_transformer(tolower) )
        corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "debatenight","debates","debate","debates2016","https","hillaryclinton","donald","trump","hillary","donaldtrump"))
        return(corpus)
}

# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweet_corpus)

head(clean_corp)
```

```{r}
## compare before and after
id<-400
tw$text[id]
clean_corp[[id]]$content
```

```{r}
# Make a term document = matrix
tweet_dtm <- TermDocumentMatrix(clean_corp)
```

## Frequent Terms Barplot
```{r}
# convert to a matrix
tweet_m <- as.matrix(tweet_dtm)

# barplot of frequent terms
term_frequency <- rowSums(tweet_m)

# Sort term_frequency in descending order
term_frequency <- sort(term_frequency,decreasing=TRUE)

# View the top 10 most common words
term_frequency[1:10]

# Plot a barchart of the most common words
barplot(term_frequency[1:20],col="tan",las=2,horiz=FALSE)

```

## Wordclouds

```{r}
library(wordcloud)

# Print the first 10 entries in term_frequency
term_frequency[1:10]

# Create word_freqs
word_freqs <- data.frame(term=names(term_frequency),num=term_frequency)

# Create a wordcloud for the values in word_freqs
wordcloud(word_freqs$term,word_freqs$num,max.words=40,colors="red")

## Add colors to the wordcloud
# Create purple_orange
purple_orange <- brewer.pal(10,"PuOr")

# Drop 2 faintest colors
purple_orange <- purple_orange[-(1:2)]

# Create a wordcloud with purple_orange palette
wordcloud(word_freqs$term,word_freqs$num,max.words=40,colors=purple_orange)

```



