---
title: "Text Analysis of Election Tweets around the First Presidential Debate"
author: "Andy Pickering"
date: "9/28/2016"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Being in the middle (or I guess thankfully near the end!) of election season, I thought it would be interesting to examine tweets about the election.

Much of this analysis applies what I learned in a DataCamp course on text-mining: <https://www.datacamp.com/courses/intro-to-text-mining-bag-of-words>.

To collect the tweets, i'm using the rtweet package. The twitter API only lets you access tweets for the previous 7 days, so i've downloaded a collection of tweets spanning the first presidential debate and saved it as a csv file. 

Note that you will need to register for a (free) token to access the Twitter API. The creator of the rtweet package created a guide to doing this here: <https://github.com/mkearney/rtweet/blob/master/vignettes/tokens.Rmd>.

The code used to download the tweets was:

```{r eval=FALSE}
library(rtweet)
# tweets with hashtag #debatenight from 9/25-9/28
tw1 <- search_tweets("#debatenight", n = 18000,since = "2016-09-25", until = "2016-09-28", token = twitter_token, lang = "en")
# save to csv file
save_as_csv(tw1,file_name='~/Tweets_Analysis/Data/debatenight_25_28')

```

Let's load one of the datasets and take a peek at it.
```{r}
tw <- read.csv('~/Tweets_Analysis/Data/debatenight_25_28.tweets.csv')
str(tw)
```

There are a lot of variables returned by the rtweet package; let's clean it up a bit by removing some columns we won't use:
```{r}
tw2<-tw[,-c(2:4,6:18,20:23)]
str(tw2)
```

More cleaning: keep only the tweets that were not retweets, and remove username mentions from the tweets: 
```{r }
suppressMessages(library(dplyr))
dim(tw2)
tw2 <- tw2 %>% filter(is_retweet == FALSE) %>%
        mutate(text = gsub("\\@.*", "", text)) 
dim(tw2)
```
We can see that this greatly reduces the number of tweets (indicating many were retweets).

## Cleaning Text and Making a Corpus

Let's take a look at the text of the first few tweets:
```{r}
head(tw2)$text
```

We need to clean up the tweets text before doing some analysis.. I'm going to use the tm package for this. Some of the cleaning includes: 

* Remove whitespace
* Remove punctuation
* Make all lowercase
* Remove common words


```{r}
library(tm)

# 1st need to interpret each element in tweets as a document
tweet_source <- VectorSource(tw2$text)

# Then make a corpus
tweet_corpus <- VCorpus(tweet_source)

# Define a function to do some basic cleaning of the corpus text
clean_corpus <- function(corpus){
        corpus <- tm_map(corpus, stripWhitespace)
        corpus <- tm_map(corpus, removePunctuation)
        corpus <- tm_map(corpus, content_transformer(tolower) )
        corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "#DebateNight","debatenight","debates","debate","debates2016","https","clinton","hillaryclinton","donald","trump","hillary","donaldtrump"))
        return(corpus)
}

# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweet_corpus)
```

### Compare a tweet before and after to illustrate cleaning effects
```{r}
id<-400
tw2$text[id]
clean_corp[[id]]$content
```

### Make a term document matrix
```{r}
# Make a term document = matrix
tweet_dtm <- TermDocumentMatrix(clean_corp)
```

## Frequent Terms Barplot
```{r}
# convert to a matrix
tweet_m <- as.matrix(tweet_dtm)

# barplot of frequent terms
term_frequency <- rowSums(tweet_m)

# Sort term_frequency in descending order
term_frequency <- sort(term_frequency,decreasing=TRUE)

# View the top 10 most common words
term_frequency[1:10]

# Plot a barchart of the most common words
barplot(term_frequency[1:20],col="tan",las=2,horiz=FALSE)

```

## Wordclouds

```{r}
library(wordcloud)

# Print the first 10 entries in term_frequency
term_frequency[1:10]

# Create word_freqs
word_freqs <- data.frame(term=names(term_frequency),num=term_frequency)

# Create a wordcloud for the values in word_freqs
wordcloud(word_freqs$term,word_freqs$num,max.words=40,colors="red")

```

### We can make this a little nicer by adding a colormap to the image.
```{r}
## Add colors to the wordcloud
# Create purple_orange
purple_orange <- brewer.pal(10,"PuOr")

# Drop 2 faintest colors
purple_orange <- purple_orange[-(1:2)]

# Create a wordcloud with purple_orange palette
wordcloud(word_freqs$term,word_freqs$num,max.words=40,colors=purple_orange)

```

### Issues/Lessons Learned
* Lots of the original tweets I gathered were retweets. Is there a way to not return retweets in the search?
* Need to remove hashtags from tweets
* Seems like there are still a lot of common words showing up in the most frequent terms and wordclouds; I will try using a tf-idf scheme to discount words that show up in most of the tweets and emphasize the differences.
* I'd also like to make a comparison cloud to contrast different groups (for example Trump supporters vs Clinton supporters, or tweets by Trump and Clinton themselves).


